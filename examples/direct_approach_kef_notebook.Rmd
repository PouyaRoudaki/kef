---
title: "Direct Approach KEF"
author: "Pouya Roudaki"
date: "2024-10-21"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Kernel Mean Embedding Estimation

Set the randomness seed.
```{r}
# set the seed
seed <- 7
set.seed(seed)

```

## Specfiy the True density
Set the mean and standard deviation of normal distribution P
```{r}
# Set the mean and standard deviation of normal distribution P
 n = 500
 means = c(-2, 2)
 sds = c(1, 1.5)
 probabilities = c(0.3, 0.7)
```

## Take Random samples
```{r}
# vector of fixed points
vec_fixed_points <- sort(rnorm_mixture(n, means, sds, probabilities))

library(ggplot2)
# Assuming vec_fixed_points is your data
df <- data.frame(points = vec_fixed_points)

# Create the histogram with 20 breaks
ggplot(df, aes(x = points, y = ..density..)) +
  geom_histogram(bins = 20, fill = "blue", color = "black") +
  labs(title = "Histogram of sampled points", x = "Values", y = "Frequency")+
  theme_bw()
```

## Find Gram Matrix of the sampled points
```{r}
# List of fixed points
list_fixed_points = as.list(vec_fixed_points)

# Find the Gram matrix
gram <- gram_matrix(vec_list = list_fixed_points)
```

### Check the eigen values

```{r}
min(eigen(gram)$values)
```

## Find the Conv Gram matrix
```{r}
conv_gram <- conv_gram_matrix(vec_list = list_fixed_points)
```

### Check the eigen values
```{r}
min(eigen(conv_gram)$values)
```
## Bandwidth selection
Apply the norm to all combinations in order to find median of them and setting the bandwidth equal to that.
```{r}
#all_norm_two <- apply(combinations, MARGIN = 1, function(pair) {
#  norm(x = list_fixed_points[[pair[2]]] - list_fixed_points[[pair[1]]], p=2)^2
#})
```



 Setting the bandwidth of kernel to be the 1 (In Gretton's paper equal to median of norm 2 distances)


```{r}
bandwidth <- 1
```

## True Kernel Mean Embeddings

```{r}
# Grid of 100 points from -10 to 10
u <- 5
grid_points <- seq(-u + means[1], u + means[length(means)], length.out = 1000)

# Kernel mean embedding becareful change the mean if change the mean of P
KME_true <- sapply(vec_fixed_points, function(point) {
  sum <- 0
  for(i in 1:length(probabilities)) {
    sum <- sum + probabilities[i] * dnorm(point, mean = means[i], sd = sqrt(bandwidth^2 + sds[i]^2))
  }
  return(bandwidth * sqrt(2 * pi) * sum)
})

# Data frame with true kernel mean embeddings
true_KME <- data.frame(vec_fixed_points, KME_true)
```

## Kernel Mean Embedding Estimation: Standard Estimator

```{r}
# Function evaluation on the grid
results <- sapply(vec_fixed_points, function(point) {
  std_est_KME(evaluate_at = point,
              list_fixed = list_fixed_points,
              kernel_type = "rbf",
              kernel_params = list(length_scale = bandwidth, degree = 2, free_add = 0, free_mult = 1, nu_matern = 1))
})
```

```{r}
# Data frame with estimated kernel mean embedding
df_std <- data.frame(vec_fixed_points, standard = results)

# Data frame for fixed points: adding e
df_fixed_points <- data.frame(x = vec_fixed_points, y = rep(0, length(vec_fixed_points)))
```

### Kernel Mean Embedding Estimator
```{r}
# Plot the results using ggplot
library(ggplot2)
# Create a combined data frame to handle both blue (standard) and orange (true KME) lines
df_combined <- rbind(
  data.frame(grid_points = df_std$vec_fixed_points, value = df_std$standard, line = "KME Estimator"),
  data.frame(grid_points = true_KME$vec_fixed_points, value = true_KME$KME_true, line = "True KME")
)

ggplot() +
  geom_hline(yintercept = 0, color = "gray", linetype = "solid", linewidth = 0.5) +  # Add axis y = 0
  geom_vline(xintercept = 0, color = "gray", linetype = "solid", linewidth = 0.5) +  # Add axis at x = 0
  geom_point(data = df_fixed_points, aes(x = x, y = y), color = 'red', size = 2 ,shape =3) +  # Plot x points with y = 0 in red
  geom_line(data = df_combined, aes(x = grid_points, y = value, color = line), linewidth = 1.2) + # Plot both lines with color mapped to 'line'
  labs(x = "x",
       y = "Standard Estimator $\\hat{\\mu}_{\\mathbb{P}}",
       title = "Standard Estimator of Kernel Mean Embedding") +
  theme_bw() +
  theme(panel.grid = element_blank(),  # Remove grid lines
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank()) +
  scale_color_manual(values = c("KME Estimator" = "blue", "True KME" = "orange")) +  # Custom colors for the legend
  guides(color = guide_legend(title = "Type"))  # Add a legend title


```

## Regularized Estimator of Kernel Mean Embedding

### Pre calculations:

```{r}
gram <- gram_matrix(vec_list = list_fixed_points)

n = length(list_fixed_points)

rho = 1/(n^2) * sum(gram)

rho_with_stroke = 1/n * sum(diag(gram))

lambda_reg = n*(rho_with_stroke-rho) / ((n-1)*(n*rho-rho_with_stroke))
```

### Find the shrinkage estimator of KME:

```{r}
# Function evaluation on the grid
results <- sapply(vec_fixed_points, function(point) {
  reg_est_KME(evaluate_at = point,
              list_fixed = list_fixed_points,
              kernel_type = "rbf",
              kernel_params = list(length_scale = bandwidth, degree = 2, free_add = 0, free_mult = 1, nu_matern = 1),
              precomputed = list(lambda = lambda_reg))
})


# Data frame with your data
df_reg <- data.frame(vec_fixed_points, regularized = results)
```

## Shrinkage Estimator of Kernel Mean Embedding ----
### Pre calculations:
```{r}
gram <- gram_matrix(vec_list = list_fixed_points)

n = length(list_fixed_points)

lambda_grid <- 10^seq(-14,10,1)

lambda_n_grid <- 10^seq(-14,10,1) * (n-1)

### Precompute regularized inverses for each lambda_n

regularized_inverse_grid <- lapply(lambda_n_grid, function(lambda_n){solve(gram + lambda_n * diag(n))})
```
### LOOCV hyper parameter selection
```{r}
loocv_values <- rep(0,length = length(lambda_grid))
### Use sapply to iterate over lambda_grid
loocv_values <- sapply(1:length(lambda_grid), function(i) {
  loocv_shr(gram = gram, lambda = lambda_grid[i], precomp_reg_inv = regularized_inverse_grid[[i]])
})

### Create a dataframe to store lambda values and their corresponding LOOCV errors
loocv_df <- data.frame(lambda = lambda_grid, loocv = loocv_values)

### Print the dataframe
print(loocv_df)
```

### CV hyper parameter selection

```{r}
lambda_grid <- c(10^seq(-14,10,1))

cv_values <- rep(0,length = length(lambda_grid))
### Use sapply to iterate over lambda_grid
cv_values <- sapply(1:length(lambda_grid), function(i) {
  cross_validation(gram = gram, lambda = lambda_grid[i], folds_number = nrow(gram),estimator_type = "shrinkage")
})

### Create a dataframe to store lambda values and their corresponding LOOCV errors
cv_df <- data.frame(lambda = lambda_grid, cv = cv_values)

### Print the dataframe
print(cv_df)

save.image("C:/Users/rouda/OneDrive/Research/Codes/R/kef/examples/rdata_direct_1.RData")
```

### Find the best hyper parameter and corresponding beta weights

```{r}
merged_num_anal <- merge(cv_df,loocv_df,by = "lambda")

# Choose the best lambda with lowest loocv
lambda_shr <- cv_df[which.min(cv_df$cv),"lambda"]

# Calculate inverse of regularizer
inverse_regularizer <- solve(gram + n * lambda_shr*diag(n))

# 1_n vector
one_n <- rep(1,n)/n

# Find beta
beta_s <- sqrt(n) * inverse_regularizer %*% gram %*%  one_n
```

### Find the shrinkage estimator of KME 

```{r}
# Function evaluation on the grid
results <- sapply(vec_fixed_points, function(point) {
  shr_est_KME(evaluate_at = point,
              list_fixed = list_fixed_points,
              lambda_tunner = 1,
              kernel_type = "rbf",
              kernel_params = list(length_scale = bandwidth, degree = 2, free_add = 0, free_mult = 1, nu_matern = 1),
              precomputed = list(beta_s = beta_s))
})


# Data frame with your data
df_shr <- data.frame(vec_fixed_points, shrinkage = results)

```


## Bayesian Estimator of Kernel Mean Embedding ----

### Pre calculations:
```{r}
tau <- 1

R <- conv_gram_matrix(vec_list = list_fixed_points,
                      kernel_type = "rbf",
                      kernel_params = list(length_scale = bandwidth, degree = 2, free_add = 0, free_mult = 1, nu_matern = 1),
                      dom_measure_type = "gaussian",
                      dom_measure_parmas = list(eta = 5))
```


```{r}
### Find number of fixed points
n <- length(list_fixed_points)

### inverse matrix of regularizer
inverse_regularizer <- solve(R + tau^2/n * diag(n))


### Function evaluation on the grid
results <- lapply(vec_fixed_points, function(point) {
  bayesian_est_KME(evaluate_at = point,
                   list_fixed = list_fixed_points,
                   kernel_type = "rbf",
                   kernel_params = list(length_scale = bandwidth, degree = 2, free_add = 0, free_mult = 1, nu_matern = 1),
                   dom_measure_type = "gaussian",
                   dom_measure_parmas = list(eta = 5),
                   tau = 1,
                   precomputed = list(inverse_regularizer = inverse_regularizer))
})

post_mean <- sapply(results, function(x) x[1])
post_var <-sapply(results, function(x) x[2])
# Data frame with your data
df_bayes <- data.frame(vec_fixed_points, post_mean, upper_conf = post_mean+sqrt(post_var), lower_conf = post_mean - sqrt(post_var))
```

## Merge all the KME estimators and plot

```{r}
df_all <- merge(df_std,df_reg,by = "vec_fixed_points")
df_all <- merge(df_all,df_shr,by = "vec_fixed_points")
df_all <- merge(df_all,df_bayes,by = "vec_fixed_points")
df_all <- merge(df_all,true_KME,by = "vec_fixed_points")

save.image("C:/Users/rouda/OneDrive/Research/Codes/R/kef/examples/rdata_direct_2.RData")
```

```{r}
load("C:/Users/rouda/OneDrive/Research/Codes/R/kef/examples/rdata_direct_2.RData")
```


```{r}
library(tidyr)

#gather data from columns 2 and 3
df_long <- gather(df_all, key="type", value="estimation", 2:8)

df_long$type <- factor(df_long$type, levels = colnames(df_all)[-1])

# Data frame for fixed points:
df_fixed_points <- data.frame(x = vec_fixed_points, y = rep(0, length(vec_fixed_points)))
```

```{r}
# Plot the results using ggplot
library(ggplot2)
ggplot(df_long, aes(x = vec_fixed_points, y = estimation, color = type)) +
  geom_hline(yintercept = 0, color = "gray", linetype = "solid", linewidth = 0.5) +  # Add axis y = 0
  geom_vline(xintercept = 0, color = "gray", linetype = "solid", linewidth = 0.5) +  # Add axis at x = 0
  geom_point(data = df_fixed_points, aes(x = x, y = y), color = 'red', size = 2 ,shape =3) +  # Plot x points with y = 0 in red
  geom_line(linewidth = 1.2) + # Plot kernel mean embedding estimator
  labs(x = "x",
       y = "Estimator $\\hat{\\mu}_{\\mathbb{P}}(x)",
       title = "Estimator of Kernel Mean Embedding")+
  theme_bw()+
  theme(panel.grid = element_blank(),  # Remove grid lines
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank()) +  # Axis ticks color
  scale_color_manual(values = c('orange', 'green', 'green3', 'darkred', 'gray',
                                'gray', 'blue')) # Customize colors
```

```{r}
library(dplyr)

# Plot the results using ggplot
ggplot(df_long %>% filter(type %in% c("standard","KME_true")), aes(x = vec_fixed_points, y = estimation, color = type)) +
  geom_hline(yintercept = 0, color = "gray", linetype = "solid", linewidth = 0.5) +  # Add axis y = 0
  geom_vline(xintercept = 0, color = "gray", linetype = "solid", linewidth = 0.5) +  # Add axis at x = 0
  geom_point(data = df_fixed_points, aes(x = x, y = y), color = 'red', size = 2 ,shape =3) +  # Plot x points with y = 0 in red
  geom_line(linewidth = 1.2) + # Plot kernel mean embedding estimator
  labs(x = "x",
       y = "Estimator $\\hat{\\mu}_{P}(x)$",
       title = "Estimator of Kernel Mean Embedding")+
  theme_bw()+
  theme(panel.grid = element_blank(),  # Remove grid lines
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank()) +  # Axis ticks color
  scale_color_manual(values = c('orange','blue')) # Customize colors
```




```{r}
# Plot the results using ggplot
ggplot(df_long %>% filter(type %in% c("regularized","KME_true")), aes(x = vec_fixed_points, y = estimation, color = type)) +
  geom_hline(yintercept = 0, color = "gray", linetype = "solid", linewidth = 0.5) +  # Add axis y = 0
  geom_vline(xintercept = 0, color = "gray", linetype = "solid", linewidth = 0.5) +  # Add axis at x = 0
  geom_point(data = df_fixed_points, aes(x = x, y = y), color = 'red', size = 2 ,shape =3) +  # Plot x points with y = 0 in red
  geom_line(linewidth = 1.2) + # Plot kernel mean embedding estimator
  labs(x = "x",
       y = "Estimator $\\hat{\\mu}_{\\mathbb{P}}(x)",
       title = "Estimator of Kernel Mean Embedding")+
  theme_bw()+
  theme(panel.grid = element_blank(),  # Remove grid lines
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank()) +  # Axis ticks color
  scale_color_manual(values = c('green','blue')) # Customize colors
```


```{r}
# Plot the results using ggplot
ggplot(df_long %>% filter(type %in% c("shrinkage","KME_true")), aes(x = vec_fixed_points, y = estimation, color = type)) +
  geom_hline(yintercept = 0, color = "gray", linetype = "solid", linewidth = 0.5) +  # Add axis y = 0
  geom_vline(xintercept = 0, color = "gray", linetype = "solid", linewidth = 0.5) +  # Add axis at x = 0
  geom_point(data = df_fixed_points, aes(x = x, y = y), color = 'red', size = 2 ,shape =3) +  # Plot x points with y = 0 in red
  geom_line(linewidth = 1.2) + # Plot kernel mean embedding estimator
  labs(x = "x",
       y = "Estimator $\\hat{\\mu}_{\\mathbb{P}}(x)",
       title = "Estimator of Kernel Mean Embedding")+
  theme_bw()+
  theme(panel.grid = element_blank(),  # Remove grid lines
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank()) +  # Axis ticks color
  scale_color_manual(values = c('green3','blue')) # Customize colors
```

```{r}
# Plot the results using ggplot
ggplot(df_long %>% filter(type %in% c("post_mean",'upper_conf','lower_conf',"KME_true")), aes(x = vec_fixed_points, y = estimation, color = type)) +
  geom_hline(yintercept = 0, color = "gray", linetype = "solid", linewidth = 0.5) +  # Add axis y = 0
  geom_vline(xintercept = 0, color = "gray", linetype = "solid", linewidth = 0.5) +  # Add axis at x = 0
  geom_point(data = df_fixed_points, aes(x = x, y = y), color = 'red', size = 2 ,shape =3) +  # Plot x points with y = 0 in red
  geom_line(linewidth = 1.2) + # Plot kernel mean embedding estimator
  labs(x = "x",
       y = "Estimator $\\hat{\\mu}_{\\mathbb{P}}(x)",
       title = "Estimator of Kernel Mean Embedding")+
  theme_bw()+
  theme(panel.grid = element_blank(),  # Remove grid lines
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank()) +  # Axis ticks color
  scale_color_manual(values = c('darkred','gray','gray','blue')) # Customize colors
```

# Find the probabilities using KME and centered kernel

 The relationship for $\hat{\mu}(\mathbf{x}_i)$ is given by:
\[
    \hat{\mu}(\mathbf{x}_i) = \sum_{j=1}^n h(\mathbf{x}_i, \mathbf{x}_j) p_\theta(\mathbf{x}_j).
\]
 Furthermore, we can estimate $\hat{\mu}(\mathbf{x}_i)$ using methods like shrinkage or Bayesian approaches:
Moreover, we can estimate $\hat{\mu}(\mathbf{x}_i)$ with methods like shrinkage methods \citep{muandet2016kernelmeanshrinkageestimators} and Bayesian method \citep{flaxman2016bayesianlearningkernelembeddings}.
So we can say $\hat{\mathbf{p}}_\theta = \mathbf{H}_{sample}^{-1} \hat{\boldsymbol{\mu}}$

 This allows us to form equations where only $\theta$'s are unknown:
So we can make equations that only $\theta$'s are unknown, and we can find these parameters using estimated kernel mean embeddings.



 Recall that the relationship for $p_\theta(\mathbf{x}_j)$ is:
\[
    \hat{p}_\theta(\mathbf{x}_j) = \dfrac{\exp(\hat{\theta}_n(\mathbf{x}_j))}{\sum_{j =1}^n \exp(\hat{\theta}_n(\mathbf{x}_j))}.
\]

## Specify the grid and centering grid
```{r}
sampled_x <- vec_fixed_points
x_grid <-  seq(-7,7,length.out = 1000)
# centering_grid <- sampled_x This doesn't work because using this centering grid the kernel mean embedding is zero.
centering_grid <- runif(min = -7,max = 7,n = 1000)
```

## Find Kernel Matrices:

$\mathbf{H}_{sample}$:
```{r}
centered_kernel_mat_at_sampled <- centered_kernel_matrix(first_vec_kernel = sampled_x,
                                                         second_vec_kernel = sampled_x,
                                                         centering_grid = centering_grid,
                                                         hurst_coef = 0.5)
```

```{r}
# Positive Definiteness of centered_kernel_mat_at_sampled

# Get the eigenvalues
eigenvalues <- eigen(centered_kernel_mat_at_sampled)$values

# Check if all eigenvalues are positive
is_positive_definite <- all(eigenvalues > 0)
is_positive_definite
```
```{r}
# Check if the matrix is symmetric
is_symmetric <- all(centered_kernel_mat_at_sampled == t(centered_kernel_mat_at_sampled))

is_symmetric
```
```{r}
all(df_reg$regularized>0)
```


$\mathbf{H}_{grid}$:
```{r}
centered_kernel_mat_at_grid <- centered_kernel_matrix(first_vec_kernel = sampled_x,
                                                         second_vec_kernel = x_grid,
                                                         centering_grid = centering_grid,
                                                         hurst_coef = 0.5)
```

## Find the probababilities:

```{r}
p <- solve(centered_kernel_mat_at_sampled) %*% df_reg$regularized

p <- p/sum(p)

plot_data <- data.frame(x = vec_fixed_points, prob = p)

ggplot(plot_data, aes(x,prob)) + geom_point(color = "blue") + theme_bw()
```



### Find ln(p):

```{r}
ln_p <- log(p)
```

# Find U weights:

```{r}
one_n <- rep(1, length(sampled_x))
one_m <- rep(1, length(x_grid))

u_vec <-  one_m %*% t(centered_kernel_mat_at_grid) %*% solve(centered_kernel_mat_at_sampled)
```

# Find the $\theta$'s:


```{r}
# Function to compute inner product ignoring NAs
inner_product_no_na <- function(vec1, vec2) {
  # Remove NA indices from both vectors
  valid_indices <- !is.na(vec1) & !is.na(vec2)
  sum(vec1[valid_indices] * vec2[valid_indices])
}

C <-  - inner_product_no_na(as.vector(u_vec),as.vector(ln_p)) / inner_product_no_na(as.vector(one_n),as.vector(ln_p))
theta <- ln_p + C
```

# Estimate the probabilities:

```{r}
centered_kernel_self_grid <- diag(centered_kernel_mat_at_sampled)

estimated_p <- exp(theta + 0.5 * centered_kernel_self_grid)


```

```{r}
sum(estimated_p,na.rm = T)
```


## Show the result:

```{r}
# Combine into a data frame
df_result <- data.frame(sampled_x, estimated_p)


# Create the ggplot
ggplot() +
  geom_point(data = df_result, aes(x = sampled_x, y = estimated_p/sum(estimated_p)), color = "red") +      # Red points for df_result
  geom_point(data = plot_data, aes(x = x, y = prob), color = "blue") +                    # Blue points for plot_data
  labs(title = "Estimated P versus Sampled X",
       x = "Sampled X", y = "Estimated P") +
  theme_bw()
```

